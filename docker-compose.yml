networks:
  lakehouse-net:
    driver: bridge

volumes:
  minio-data:
  postgres-metastore-data:
  postgres-airflow-data:

x-airflow-common: &airflow-common
  build:
    context: ./airflow
    dockerfile: Dockerfile
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_AIRFLOW_USER}:${POSTGRES_AIRFLOW_PASSWORD}@postgres-airflow:5432/${POSTGRES_AIRFLOW_DB}
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
    AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
    AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: ${AIRFLOW__WEBSERVER__EXPOSE_CONFIG}
    AIRFLOW_UID: ${AIRFLOW_UID}
    # MinIO/S3 credentials
    AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
    AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    MINIO_ENDPOINT: http://${MINIO_ENDPOINT}
    # dbt
    DBT_PROFILES_DIR: ${DBT_PROFILES_DIR}
    DBT_PROJECT_DIR: ${DBT_PROJECT_DIR}
    DBT_TARGET: ${DBT_TARGET}
    # Trino connection
    TRINO_HOST: trino-coordinator
    TRINO_PORT: ${TRINO_COORDINATOR_PORT}
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./dbt:/opt/airflow/dbt
    - ./scripts:/opt/airflow/scripts
    - /var/run/docker.sock:/var/run/docker.sock
  user: "${AIRFLOW_UID}:0"
  depends_on:
    postgres-airflow:
      condition: service_healthy
    minio:
      condition: service_healthy
    hive-metastore:
      condition: service_healthy
    trino-coordinator:
      condition: service_started
  networks:
    - lakehouse-net

services:
  # ============================================================================
  # MinIO - S3-compatible Object Storage
  # ============================================================================
  minio:
    image: minio/minio:RELEASE.2024-01-16T16-07-38Z
    container_name: minio
    hostname: minio
    ports:
      - "${MINIO_API_PORT}:9000"
      - "${MINIO_CONSOLE_PORT}:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9000' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - lakehouse-net

  # MinIO Client - Initialize buckets
  minio-mc:
    image: minio/mc:RELEASE.2024-01-13T08-44-48Z
    container_name: minio-mc
    depends_on:
      minio:
        condition: service_healthy
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_ENDPOINT: http://minio:9000
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      /usr/bin/mc mb --ignore-existing myminio/lakehouse;
      /usr/bin/mc anonymous set download myminio/lakehouse;
      echo 'MinIO lakehouse bucket initialized successfully';
      "
    networks:
      - lakehouse-net

  # ============================================================================
  # PostgreSQL - Hive Metastore Backend
  # ============================================================================
  postgres-metastore:
    image: postgres:15.5-alpine
    container_name: postgres-metastore
    hostname: postgres-metastore
    ports:
      - "${POSTGRES_METASTORE_PORT}:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_METASTORE_USER}
      POSTGRES_PASSWORD: ${POSTGRES_METASTORE_PASSWORD}
      POSTGRES_DB: ${POSTGRES_METASTORE_DB}
    volumes:
      - postgres-metastore-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_METASTORE_USER} -d ${POSTGRES_METASTORE_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - lakehouse-net

  # ============================================================================
  # Hive Metastore - Metadata Catalog
  # ============================================================================
  hive-metastore:
    build:
      context: ./metastore
      dockerfile: Dockerfile
    container_name: hive-metastore
    hostname: hive-metastore
    ports:
      - "${HIVE_METASTORE_PORT}:9083"
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      SERVICE_OPTS: "-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver 
                     -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-metastore:5432/${POSTGRES_METASTORE_DB} 
                     -Djavax.jdo.option.ConnectionUserName=${POSTGRES_METASTORE_USER} 
                     -Djavax.jdo.option.ConnectionPassword=${POSTGRES_METASTORE_PASSWORD}"
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    volumes:
      - ./metastore/metastore-site.xml:/opt/hive/conf/metastore-site.xml
      - ./metastore/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
    depends_on:
      postgres-metastore:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9083' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s
    networks:
      - lakehouse-net

  # ============================================================================
  # PostgreSQL - Airflow Backend
  # ============================================================================
  postgres-airflow:
    image: postgres:15.5-alpine
    container_name: postgres-airflow
    hostname: postgres-airflow
    ports:
      - "${POSTGRES_AIRFLOW_PORT}:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_AIRFLOW_USER}
      POSTGRES_PASSWORD: ${POSTGRES_AIRFLOW_PASSWORD}
      POSTGRES_DB: ${POSTGRES_AIRFLOW_DB}
    volumes:
      - postgres-airflow-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_AIRFLOW_USER} -d ${POSTGRES_AIRFLOW_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - lakehouse-net

  # ============================================================================
  # Airflow - Orchestration
  # ============================================================================
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    user: "0:0"
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins}
        exec /entrypoint airflow db init
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "${AIRFLOW_WEBSERVER_PORT}:8080"
      - "8082:8082"  # dbt docs
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  # ============================================================================
  # Trino - Distributed SQL Query Engine
  # ============================================================================
  trino-coordinator:
    image: trinodb/trino:438
    container_name: trino-coordinator
    hostname: trino-coordinator
    ports:
      - "${TRINO_COORDINATOR_PORT}:8080"
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    volumes:
      - ./trino/coordinator/config.properties:/etc/trino/config.properties
      - ./trino/coordinator/jvm.config:/etc/trino/jvm.config
      - ./trino/coordinator/log.properties:/etc/trino/log.properties
      - ./trino/coordinator/node.properties:/etc/trino/node.properties
      - ./trino/catalog:/etc/trino/catalog
    depends_on:
      hive-metastore:
        condition: service_healthy
    networks:
      - lakehouse-net

  trino-worker:
    image: trinodb/trino:438
    hostname: trino-worker
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    volumes:
      - ./trino/worker/config.properties:/etc/trino/config.properties
      - ./trino/worker/jvm.config:/etc/trino/jvm.config
      - ./trino/worker/log.properties:/etc/trino/log.properties
      - ./trino/worker/node.properties:/etc/trino/node.properties
      - ./trino/catalog:/etc/trino/catalog
    depends_on:
      - trino-coordinator
    networks:
      - lakehouse-net

  # ============================================================================
  # Spark Cluster - Data Processing Engine for Ingestion
  # ============================================================================
  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8083:8080"  # Spark Master UI
      - "7077:7077"  # Spark Master Port
    environment:
      SPARK_MODE: master
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      SPARK_MASTER_HOST: 0.0.0.0
      SPARK_MASTER_PORT: 7077
    volumes:
      - ./scripts:/opt/scripts
      - ./spark/conf:/opt/spark/conf
    command: >
      bash -c "
      mkdir -p /opt/spark/logs &&
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master --host 0.0.0.0 --port 7077 --webui-port 8080 > /opt/spark/logs/spark-master.log 2>&1 &
      sleep infinity
      "
    depends_on:
      - hive-metastore
      - minio
    networks:
      - lakehouse-net

  spark-worker-1:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker-1
    hostname: spark-worker-1
    ports:
      - "8084:8081"  # Worker 1 UI
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    volumes:
      - ./scripts:/opt/scripts
      - ./spark/conf:/opt/spark/conf
    command: >
      bash -c "
      mkdir -p /opt/spark/logs &&
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --cores 2 --memory 2g --webui-port 8081 > /opt/spark/logs/spark-worker-1.log 2>&1 &
      sleep infinity
      "
    depends_on:
      - spark-master
    networks:
      - lakehouse-net

  spark-worker-2:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker-2
    hostname: spark-worker-2
    ports:
      - "8085:8081"  # Worker 2 UI
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    volumes:
      - ./scripts:/opt/scripts
      - ./spark/conf:/opt/spark/conf
    command: >
      bash -c "
      mkdir -p /opt/spark/logs &&
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --cores 2 --memory 2g --webui-port 8081 > /opt/spark/logs/spark-worker-2.log 2>&1 &
      sleep infinity
      "
    depends_on:
      - spark-master
    networks:
      - lakehouse-net
